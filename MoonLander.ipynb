{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chandlerbing65nm/Moon-Lander-with-Expected-Sarsa/blob/main/MoonLander.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4qP3T3Hs37P"
      },
      "source": [
        "## Import and Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "TZwtupJLs37S"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from rl_glue import RLGlue\n",
        "from environment import BaseEnvironment\n",
        "from lander_env import MoonLanderEnvironment\n",
        "from agent import BaseAgent\n",
        "from collections import deque\n",
        "from copy import deepcopy\n",
        "from tqdm import tqdm\n",
        "import os \n",
        "import shutil\n",
        "from plot_script import plot_result\n",
        "from dummy_environment import DummyEnvironment\n",
        "from dummy_agent import DummyAgent"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym[box2d]"
      ],
      "metadata": {
        "id": "Hpuiy6iwuqaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XAmWc-9s37U"
      },
      "source": [
        "## Action-Values Neural Network\n",
        "\n",
        "We used a neural network for approximating the action-value function in a control problem. The main difference between approximating a state-value function and an action-value function using a neural network is that in the former the output layer only includes one unit whereas in the latter the output layer includes as many units as the number of actions. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6OdX93SSs37V"
      },
      "outputs": [],
      "source": [
        "class ActionValueNetwork:\n",
        " \n",
        "    def __init__(self, network_config):\n",
        "        self.state_dim = network_config.get(\"state_dim\")\n",
        "        self.num_hidden_units = network_config.get(\"num_hidden_units\")\n",
        "        self.num_actions = network_config.get(\"num_actions\")\n",
        "        \n",
        "        self.rand_generator = np.random.RandomState(network_config.get(\"seed\"))\n",
        "        \n",
        "        # Specifies self.layer_sizes which shows the number of nodes in each layer\n",
        "        self.layer_sizes = np.array([self.state_dim, self.num_hidden_units, self.num_actions]) \n",
        "        \n",
        "        # Initializes the weights of the neural network\n",
        "        # self.weights is an array of dictionaries with each dictionary corresponding to \n",
        "        # the weights from one layer to the next. Each dictionary includes W and b\n",
        "        self.weights = [dict() for i in range(0, len(self.layer_sizes) - 1)]\n",
        "        for i in range(0, len(self.layer_sizes) - 1):\n",
        "            self.weights[i]['W'] = self.init_saxe(self.layer_sizes[i], self.layer_sizes[i + 1])\n",
        "            self.weights[i]['b'] = np.zeros((1, self.layer_sizes[i + 1]))\n",
        "    \n",
        "    def get_action_values(self, s):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            s (Numpy array): The state.\n",
        "        Returns:\n",
        "            The action-values (Numpy array) calculated using the network's weights.\n",
        "        \"\"\"\n",
        "        \n",
        "        W0, b0 = self.weights[0]['W'], self.weights[0]['b']\n",
        "        psi = np.dot(s, W0) + b0\n",
        "        x = np.maximum(psi, 0)\n",
        "        \n",
        "        W1, b1 = self.weights[1]['W'], self.weights[1]['b']\n",
        "        q_vals = np.dot(x, W1) + b1\n",
        "\n",
        "        return q_vals\n",
        "    \n",
        "    def get_TD_update(self, s, delta_mat):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            s (Numpy array): The state.\n",
        "            delta_mat (Numpy array): A 2D array of shape (batch_size, num_actions). Each row of delta_mat  \n",
        "            correspond to one state in the batch. Each row has only one non-zero element \n",
        "            which is the TD-error corresponding to the action taken.\n",
        "        Returns:\n",
        "            The TD update (Array of dictionaries with gradient times TD errors) for the network's weights\n",
        "        \"\"\"\n",
        "\n",
        "        W0, b0 = self.weights[0]['W'], self.weights[0]['b']\n",
        "        W1, b1 = self.weights[1]['W'], self.weights[1]['b']\n",
        "        \n",
        "        psi = np.dot(s, W0) + b0\n",
        "        x = np.maximum(psi, 0)\n",
        "        dx = (psi > 0).astype(float)\n",
        "\n",
        "        # td_update has the same structure as self.weights, that is an array of dictionaries.\n",
        "        # td_update[0][\"W\"], td_update[0][\"b\"], td_update[1][\"W\"], and td_update[1][\"b\"] have the same shape as \n",
        "        # self.weights[0][\"W\"], self.weights[0][\"b\"], self.weights[1][\"W\"], and self.weights[1][\"b\"] respectively\n",
        "        td_update = [dict() for i in range(len(self.weights))]\n",
        "         \n",
        "        v = delta_mat\n",
        "        td_update[1]['W'] = np.dot(x.T, v) * 1. / s.shape[0]\n",
        "        td_update[1]['b'] = np.sum(v, axis=0, keepdims=True) * 1. / s.shape[0]\n",
        "        \n",
        "        v = np.dot(v, W1.T) * dx\n",
        "        td_update[0]['W'] = np.dot(s.T, v) * 1. / s.shape[0]\n",
        "        td_update[0]['b'] = np.sum(v, axis=0, keepdims=True) * 1. / s.shape[0]\n",
        "                \n",
        "        return td_update\n",
        "    \n",
        "    # Weight initialization\n",
        "    # (Derivations to the nonlinear dynamics of learning in deep linear neural networks by Saxe, A et al., 2013)\n",
        "    def init_saxe(self, rows, cols):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            rows (int): number of input units for layer.\n",
        "            cols (int): number of output units for layer.\n",
        "        Returns:\n",
        "            NumPy Array consisting of weights for the layer based on the initialization in Saxe et al.\n",
        "        \"\"\"\n",
        "        tensor = self.rand_generator.normal(0, 1, (rows, cols))\n",
        "        if rows < cols:\n",
        "            tensor = tensor.T\n",
        "        tensor, r = np.linalg.qr(tensor)\n",
        "        d = np.diag(r, 0)\n",
        "        ph = np.sign(d)\n",
        "        tensor *= ph\n",
        "\n",
        "        if rows < cols:\n",
        "            tensor = tensor.T\n",
        "        return tensor\n",
        "    \n",
        "    def get_weights(self):\n",
        "        \"\"\"\n",
        "        Returns: \n",
        "            A copy of the current weights of this network.\n",
        "        \"\"\"\n",
        "        return deepcopy(self.weights)\n",
        "    \n",
        "    def set_weights(self, weights):\n",
        "        \"\"\"\n",
        "        Args: \n",
        "            weights (list of dictionaries): Consists of weights that this network will set as its own weights.\n",
        "        \"\"\"\n",
        "        self.weights = deepcopy(weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJbr4Ny1s37W"
      },
      "source": [
        "Testing the implementation of the `__init__()` function for ActionValueNetwork:\n",
        "\n",
        "**Expected output:** (assuming no changes to the previous cells)\n",
        "\n",
        "    layer_sizes: [ 5 20  3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJWG10eGs37X",
        "outputId": "a242156b-374c-4222-dcf1-678195fc4809"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layer_sizes: [ 5 20  3]\n"
          ]
        }
      ],
      "source": [
        "network_config = {\n",
        "    \"state_dim\": 5,\n",
        "    \"num_hidden_units\": 20,\n",
        "    \"num_actions\": 3\n",
        "}\n",
        "\n",
        "test_network = ActionValueNetwork(network_config)\n",
        "print(\"layer_sizes:\", test_network.layer_sizes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnOJ6IrOs37Y"
      },
      "source": [
        "## Adam Optimizer\n",
        "\n",
        "The Adam algorithm improves the SGD update with two concepts: adaptive vector stepsizes and momentum. It keeps running estimates of the mean and second moment of the updates, denoted by $\\mathbf{m}$ and $\\mathbf{v}$ respectively:\n",
        "$$\\mathbf{m_t} = \\beta_m \\mathbf{m_{t-1}} + (1 - \\beta_m)g_t \\\\\n",
        "\\mathbf{v_t} = \\beta_v \\mathbf{v_{t-1}} + (1 - \\beta_v)g^2_t\n",
        "$$\n",
        "\n",
        "Here, $\\beta_m$ and $\\beta_v$ are fixed parameters controlling the linear combinations above and $g_t$ is the update at time $t$ (generally the gradients, but here the TD error times the gradients).\n",
        "\n",
        "Given that $\\mathbf{m}$ and $\\mathbf{v}$ are initialized to zero, they are biased toward zero. To get unbiased estimates of the mean and second moment, Adam defines $\\mathbf{\\hat{m}}$ and $\\mathbf{\\hat{v}}$ as:\n",
        "$$ \\mathbf{\\hat{m}_t} = \\frac{\\mathbf{m_t}}{1 - \\beta_m^t} \\\\\n",
        "\\mathbf{\\hat{v}_t} = \\frac{\\mathbf{v_t}}{1 - \\beta_v^t}\n",
        "$$\n",
        "\n",
        "The weights are then updated as follows:\n",
        "$$ \\mathbf{w_t} = \\mathbf{w_{t-1}} + \\frac{\\alpha}{\\sqrt{\\mathbf{\\hat{v}_t}}+\\epsilon} \\mathbf{\\hat{m}_t}\n",
        "$$\n",
        "\n",
        "Here, $\\alpha$ is the step size parameter and $\\epsilon$ is another small parameter to keep the denominator from being zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nhPMsimgs37Y"
      },
      "outputs": [],
      "source": [
        "class Adam():\n",
        "    # Initialization for self.m and self.v.\n",
        "    def __init__(self, layer_sizes, \n",
        "                 optimizer_info):\n",
        "        self.layer_sizes = layer_sizes\n",
        "\n",
        "        # Adam algorithm's hyper parameters\n",
        "        self.step_size = optimizer_info.get(\"step_size\")\n",
        "        self.beta_m = optimizer_info.get(\"beta_m\")\n",
        "        self.beta_v = optimizer_info.get(\"beta_v\")\n",
        "        self.epsilon = optimizer_info.get(\"epsilon\")\n",
        "        \n",
        "        # Initializes Adam algorithm's m and v\n",
        "        self.m = [dict() for i in range(1, len(self.layer_sizes))]\n",
        "        self.v = [dict() for i in range(1, len(self.layer_sizes))]\n",
        "        \n",
        "        for i in range(0, len(self.layer_sizes) - 1):\n",
        "            # Initialization for m and v look very much like the initializations of the weights\n",
        "            # except for the fact that initialization here is to zeroes.\n",
        "            \n",
        "            self.m[i][\"W\"] = np.zeros((self.layer_sizes[i], self.layer_sizes[i+1]))\n",
        "            self.m[i][\"b\"] = np.zeros((1, self.layer_sizes[i+1]))\n",
        "            self.v[i][\"W\"] = np.zeros((self.layer_sizes[i], self.layer_sizes[i+1]))\n",
        "            self.v[i][\"b\"] = np.zeros((1, self.layer_sizes[i+1]))\n",
        "            \n",
        "            \n",
        "            \n",
        "        # To calculate m_hat and v_hat, we use powers of beta_m and beta_v to the time step t.\n",
        "        self.beta_m_product = self.beta_m\n",
        "        self.beta_v_product = self.beta_v\n",
        "    \n",
        "    def update_weights(self, weights, td_errors_times_gradients):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            weights (Array of dictionaries): The weights of the neural network.\n",
        "            td_errors_times_gradients (Array of dictionaries): The gradient of the \n",
        "            action-values with respect to the network's weights times the TD-error\n",
        "        Returns:\n",
        "            The updated weights (Array of dictionaries).\n",
        "        \"\"\"\n",
        "        for i in range(len(weights)):\n",
        "            for param in weights[i].keys():\n",
        "                # Computes how much the weights should be incremented by.\n",
        "                \n",
        "                self.m[i][param] = self.beta_m * self.m[i][param] + (1 - self.beta_m) * td_errors_times_gradients[i][param]\n",
        "                self.v[i][param] = self.beta_v * self.v[i][param] + (1 - self.beta_v) * (td_errors_times_gradients[i][param] * td_errors_times_gradients[i][param])\n",
        "                m_hat = self.m[i][param] / (1 - self.beta_m_product)\n",
        "                v_hat = self.v[i][param] / (1 - self.beta_v_product)\n",
        "                \n",
        "                weight_update = self.step_size * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
        "                \n",
        "                weights[i][param] = weights[i][param] + weight_update\n",
        "        # To calculate m_hat and v_hat, we use powers of beta_m and beta_v to update self.beta_m_product and self.beta_v_product\n",
        "        self.beta_m_product *= self.beta_m\n",
        "        self.beta_v_product *= self.beta_v\n",
        "        \n",
        "        return weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF8IT_EDs37Z"
      },
      "source": [
        "Testing the implementation of the `__init__()` function:\n",
        "\n",
        "**Expected output:**\n",
        "\n",
        "    m[0][\"W\"] shape: (5, 2)\n",
        "    m[0][\"b\"] shape: (1, 2)\n",
        "    m[1][\"W\"] shape: (2, 3)\n",
        "    m[1][\"b\"] shape: (1, 3) \n",
        "\n",
        "    v[0][\"W\"] shape: (5, 2)\n",
        "    v[0][\"b\"] shape: (1, 2)\n",
        "    v[1][\"W\"] shape: (2, 3)\n",
        "    v[1][\"b\"] shape: (1, 3) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-juNs1Hs37a",
        "outputId": "5605fae6-24cd-49ce-a385-dd6a26636566"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "m[0][\"W\"] shape: (5, 2)\n",
            "m[0][\"b\"] shape: (1, 2)\n",
            "m[1][\"W\"] shape: (2, 3)\n",
            "m[1][\"b\"] shape: (1, 3) \n",
            "\n",
            "v[0][\"W\"] shape: (5, 2)\n",
            "v[0][\"b\"] shape: (1, 2)\n",
            "v[1][\"W\"] shape: (2, 3)\n",
            "v[1][\"b\"] shape: (1, 3) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "network_config = {\"state_dim\": 5,\n",
        "                  \"num_hidden_units\": 2,\n",
        "                  \"num_actions\": 3\n",
        "                 }\n",
        "\n",
        "optimizer_info = {\"step_size\": 0.1,\n",
        "                  \"beta_m\": 0.99,\n",
        "                  \"beta_v\": 0.999,\n",
        "                  \"epsilon\": 0.0001\n",
        "                 }\n",
        "\n",
        "network = ActionValueNetwork(network_config)\n",
        "test_adam = Adam(network.layer_sizes, optimizer_info)\n",
        "\n",
        "print(\"m[0][\\\"W\\\"] shape: {}\".format(test_adam.m[0][\"W\"].shape))\n",
        "print(\"m[0][\\\"b\\\"] shape: {}\".format(test_adam.m[0][\"b\"].shape))\n",
        "print(\"m[1][\\\"W\\\"] shape: {}\".format(test_adam.m[1][\"W\"].shape))\n",
        "print(\"m[1][\\\"b\\\"] shape: {}\".format(test_adam.m[1][\"b\"].shape), \"\\n\")\n",
        "\n",
        "assert(np.allclose(test_adam.m[0][\"W\"].shape, np.array([5, 2])))\n",
        "assert(np.allclose(test_adam.m[0][\"b\"].shape, np.array([1, 2])))\n",
        "assert(np.allclose(test_adam.m[1][\"W\"].shape, np.array([2, 3])))\n",
        "assert(np.allclose(test_adam.m[1][\"b\"].shape, np.array([1, 3])))\n",
        "\n",
        "print(\"v[0][\\\"W\\\"] shape: {}\".format(test_adam.v[0][\"W\"].shape))\n",
        "print(\"v[0][\\\"b\\\"] shape: {}\".format(test_adam.v[0][\"b\"].shape))\n",
        "print(\"v[1][\\\"W\\\"] shape: {}\".format(test_adam.v[1][\"W\"].shape))\n",
        "print(\"v[1][\\\"b\\\"] shape: {}\".format(test_adam.v[1][\"b\"].shape), \"\\n\")\n",
        "\n",
        "assert(np.allclose(test_adam.v[0][\"W\"].shape, np.array([5, 2])))\n",
        "assert(np.allclose(test_adam.v[0][\"b\"].shape, np.array([1, 2])))\n",
        "assert(np.allclose(test_adam.v[1][\"W\"].shape, np.array([2, 3])))\n",
        "assert(np.allclose(test_adam.v[1][\"b\"].shape, np.array([1, 3])))\n",
        "\n",
        "assert(np.all(test_adam.m[0][\"W\"]==0))\n",
        "assert(np.all(test_adam.m[0][\"b\"]==0))\n",
        "assert(np.all(test_adam.m[1][\"W\"]==0))\n",
        "assert(np.all(test_adam.m[1][\"b\"]==0))\n",
        "\n",
        "assert(np.all(test_adam.v[0][\"W\"]==0))\n",
        "assert(np.all(test_adam.v[0][\"b\"]==0))\n",
        "assert(np.all(test_adam.v[1][\"W\"]==0))\n",
        "assert(np.all(test_adam.v[1][\"b\"]==0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS3-Ebacs37a"
      },
      "source": [
        "## Experience Replay Buffers\n",
        "\n",
        "Experience replay is a simple method that can get some of the advantages of Dyna by saving a buffer of experience and using the data stored in the buffer as a model. This view of prior data as a model works because the data represents actual transitions from the underlying MDP. Furthermore, as a side note, this kind of model that is not learned and simply a collection of experience can be called non-parametric as it can be ever-growing as opposed to a parametric model where the transitions are learned to be represented with a fixed set of parameters or weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4cvffizqs37b"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, size, minibatch_size, seed):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            size (integer): The size of the replay buffer.              \n",
        "            minibatch_size (integer): The sample size.\n",
        "            seed (integer): The seed for the random number generator. \n",
        "        \"\"\"\n",
        "        self.buffer = []\n",
        "        self.minibatch_size = minibatch_size\n",
        "        self.rand_generator = np.random.RandomState(seed)\n",
        "        self.max_size = size\n",
        "\n",
        "    def append(self, state, action, reward, terminal, next_state):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            state (Numpy array): The state.              \n",
        "            action (integer): The action.\n",
        "            reward (float): The reward.\n",
        "            terminal (integer): 1 if the next state is a terminal state and 0 otherwise.\n",
        "            next_state (Numpy array): The next state.           \n",
        "        \"\"\"\n",
        "        if len(self.buffer) == self.max_size:\n",
        "            del self.buffer[0]\n",
        "        self.buffer.append([state, action, reward, terminal, next_state])\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            A list of transition tuples including state, action, reward, terinal, and next_state\n",
        "        \"\"\"\n",
        "        idxs = self.rand_generator.choice(np.arange(len(self.buffer)), size=self.minibatch_size)\n",
        "        return [self.buffer[idx] for idx in idxs]\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDbWpdQHs37b"
      },
      "source": [
        "## Softmax Policy\n",
        "\n",
        "One advantage of a softmax policy is that it explores according to the action-values, meaning that an action with a moderate value has a higher chance of getting selected compared to an action with a lower value. Contrast this with an $\\epsilon$-greedy policy which does not consider the individual action values when choosing an exploratory action in a state and instead chooses randomly when doing so.\n",
        "\n",
        "The probability of selecting each action according to the softmax policy is shown below:\n",
        "$$Pr{(A_t=a | S_t=s)} \\hspace{0.1cm} \\dot{=} \\hspace{0.1cm} \\frac{e^{Q(s, a)/\\tau}}{\\sum_{b \\in A}e^{Q(s, b)/\\tau}}$$\n",
        "where $\\tau$ is the temperature parameter which controls how much the agent focuses on the highest valued actions. The smaller the temperature, the more the agent selects the greedy action. Conversely, when the temperature is high, the agent selects among actions more uniformly random.\n",
        "\n",
        "Given that a softmax policy exponentiates action values, if those values are large, exponentiating them could get very large. To implement the softmax policy in a numerically stable way, we often subtract the maximum action-value from the action-values. If we do so, the probability of selecting each action looks as follows:\n",
        "\n",
        "$$Pr{(A_t=a | S_t=s)} \\hspace{0.1cm} \\dot{=} \\hspace{0.1cm} \\frac{e^{Q(s, a)/\\tau - max_{c}Q(s, c)/\\tau}}{\\sum_{b \\in A}e^{Q(s, b)/\\tau - max_{c}Q(s, c)/\\tau}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LcG1K0nPs37c"
      },
      "outputs": [],
      "source": [
        "def softmax(action_values, tau=1.0):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        action_values (Numpy array): A 2D array of shape (batch_size, num_actions). \n",
        "                       The action-values computed by an action-value network.              \n",
        "        tau (float): The temperature parameter scalar.\n",
        "    Returns:\n",
        "        A 2D array of shape (batch_size, num_actions). Where each column is a probability distribution over\n",
        "        the actions representing the policy.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Computes the preferences by dividing the action-values by the temperature parameter tau\n",
        "    preferences = action_values / tau\n",
        "    # Computes the maximum preference across the actions\n",
        "    max_preference = np.max(preferences, 1)\n",
        "\n",
        "    \n",
        "    # Reshapes max_preference array which has shape [Batch,] to [Batch, 1]. This allows NumPy broadcasting \n",
        "    # when subtracting the maximum preference from the preference of each action.\n",
        "    reshaped_max_preference = max_preference.reshape((-1, 1))\n",
        "    \n",
        "    # Computes the numerator, i.e., the exponential of the preference - the max preference.\n",
        "    exp_preferences = np.exp(preferences - reshaped_max_preference)\n",
        "    # Compute the denominator, i.e., the sum over the numerator along the actions axis.\n",
        "    sum_of_exp_preferences = np.sum(exp_preferences, 1)\n",
        "    \n",
        "    \n",
        "    # Reshapes sum_of_exp_preferences array which has shape [Batch,] to [Batch, 1] to  allow for NumPy broadcasting \n",
        "    # when dividing the numerator by the denominator.\n",
        "    reshaped_sum_of_exp_preferences = sum_of_exp_preferences.reshape((-1, 1))\n",
        "    \n",
        "    # Computes the action probabilities according to the equation in the previous cell.\n",
        "    action_probs = exp_preferences / reshaped_sum_of_exp_preferences\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    # squeeze() removes any singleton dimensions. It is used here because this function is used in the \n",
        "    # agent policy when selecting an action (for which the batch dimension is 1.) As np.random.choice is used in \n",
        "    # the agent policy and it expects 1D arrays, we need to remove this singleton batch dimension.\n",
        "    action_probs = action_probs.squeeze()\n",
        "    return action_probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pejHIflrs37c"
      },
      "source": [
        "Testing the implementation of the `softmax()` function:\n",
        "\n",
        "**Expected output:**\n",
        "\n",
        "    action_probs [[0.25849645 0.01689625 0.05374514 0.67086216]\n",
        "     [0.84699852 0.00286345 0.13520063 0.01493741]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bacz874vs37c",
        "outputId": "3ca43817-f875-43eb-eb8f-4b0acf47eeb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action_probs [[0.25849645 0.01689625 0.05374514 0.67086216]\n",
            " [0.84699852 0.00286345 0.13520063 0.01493741]]\n"
          ]
        }
      ],
      "source": [
        "rand_generator = np.random.RandomState(0)\n",
        "action_values = rand_generator.normal(0, 1, (2, 4))\n",
        "tau = 0.5\n",
        "\n",
        "action_probs = softmax(action_values, tau)\n",
        "print(\"action_probs\", action_probs)\n",
        "\n",
        "assert(np.allclose(action_probs, np.array([\n",
        "    [0.25849645, 0.01689625, 0.05374514, 0.67086216],\n",
        "    [0.84699852, 0.00286345, 0.13520063, 0.01493741]\n",
        "])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgN-Q8QEs37d"
      },
      "source": [
        "## Lunar Lander with Expected Sarsa (putting all together)\n",
        "\n",
        "At time $t$, we have an action-value function represented as a neural network, say $Q_t$. We want to update our action-value function and get a new one we can use at the next timestep. We will get this $Q_{t+1}$ using multiple replay steps that each result in an intermediate action-value function $Q_{t+1}^{i}$ where $i$ indexes which replay step we are at.\n",
        "\n",
        "In each replay step, we sample a batch of experiences from the replay buffer and compute a minibatch Expected-SARSA update. Across these N replay steps, we will use the current \"un-updated\" action-value network at time $t$, $Q_t$, for computing the action-values of the next-states. This contrasts using the most recent action-values from the last replay step $Q_{t+1}^{i}$. We make this choice to have targets that are stable across replay steps. Here is the pseudocode for performing the updates:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "& Q_t \\leftarrow \\text{action-value network at timestep t (current action-value network)}\\\\\n",
        "& \\text{Initialize } Q_{t+1}^1 \\leftarrow Q_t\\\\\n",
        "& \\text{For } i \\text{ in } [1, ..., N] \\text{ (i.e. N} \\text{  replay steps)}:\\\\\n",
        "& \\hspace{1cm} s, a, r, t, s'\n",
        "\\leftarrow \\text{Sample batch of experiences from experience replay buffer} \\\\\n",
        "& \\hspace{1cm} \\text{Do Expected Sarsa update with } Q_t: Q_{t+1}^{i+1}(s, a) \\leftarrow Q_{t+1}^{i}(s, a) + \\alpha \\cdot \\left[r + \\gamma \\left(\\sum_{b} \\pi(b | s') Q_t(s', b)\\right) - Q_{t+1}^{i}(s, a)\\right]\\\\\n",
        "& \\hspace{1.5cm} \\text{ making sure to add the } \\gamma \\left(\\sum_{b} \\pi(b | s') Q_t(s', b)\\right) \\text{ for non-terminal transitions only.} \\\\\n",
        "& \\text{After N replay steps, we set } Q_{t+1}^{N} \\text{ as } Q_{t+1} \\text{ and have a new } Q_{t+1} \\text{for time step } t + 1 \\text{ that we will fix in the next set of updates. }\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "In the pseudocode, after sampling a batch of experiences, we do many computations. The basic idea however is that we are looking to compute a form of a TD error. In order to so, we can take the following steps:\n",
        "- compute the action-values for the next states using the action-value network $Q_{t}$,\n",
        "- compute the policy $\\pi(b | s')$ induced by the action-values $Q_{t}$ (using the softmax function you implemented before),\n",
        "- compute the Expected sarsa targets $r + \\gamma \\left(\\sum_{b} \\pi(b | s') Q_t(s', b)\\right)$,\n",
        "- compute the action-values for the current states using the latest $Q_{t + 1}$, and,\n",
        "- compute the TD-errors with the Expected Sarsa targets.\n",
        " \n",
        "For the third step above, we start by computing $\\pi(b | s') Q_t(s', b)$ followed by summation to get $\\hat{v}_\\pi(s') = \\left(\\sum_{b} \\pi(b | s') Q_t(s', b)\\right)$. $\\hat{v}_\\pi(s')$ is an estimate of the value of the next state. Note for terminal next states, $\\hat{v}_\\pi(s') = 0$. Finally, we add the rewards to the discount times $\\hat{v}_\\pi(s')$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "g2KsGmZYs37d"
      },
      "outputs": [],
      "source": [
        "def get_td_error(states, next_states, actions, rewards, discount, terminals, network, current_q, tau):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        states (Numpy array): The batch of states with the shape (batch_size, state_dim).\n",
        "        next_states (Numpy array): The batch of next states with the shape (batch_size, state_dim).\n",
        "        actions (Numpy array): The batch of actions with the shape (batch_size,).\n",
        "        rewards (Numpy array): The batch of rewards with the shape (batch_size,).\n",
        "        discount (float): The discount factor.\n",
        "        terminals (Numpy array): The batch of terminals with the shape (batch_size,).\n",
        "        network (ActionValueNetwork): The latest state of the network that is getting replay updates.\n",
        "        current_q (ActionValueNetwork): The fixed network used for computing the targets, \n",
        "                                        and particularly, the action-values at the next-states.\n",
        "    Returns:\n",
        "        The TD errors (Numpy array) for actions taken, of shape (batch_size,)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Here network is the latest state of the network that is getting replay updates. In other words, \n",
        "    # the network represents Q_{t+1}^{i} whereas current_q represents Q_t, the fixed network used for computing the \n",
        "    # targets, and particularly, the action-values at the next-states.\n",
        "    \n",
        "    # Computes action values at next states using current_q network\n",
        "    # q_next_mat is a 2D array of shape (batch_size, num_actions)\n",
        "    \n",
        "\n",
        "    q_next_mat = None\n",
        "    q_next_mat = current_q.get_action_values(next_states)\n",
        "    \n",
        "    # Computes policy at next state by passing the action-values in q_next_mat to softmax()\n",
        "    # probs_mat is a 2D array of shape (batch_size, num_actions)\n",
        "    \n",
        "\n",
        "    probs_mat = None\n",
        "    probs_mat = softmax(q_next_mat, tau)\n",
        "    \n",
        "    # Computes the estimate of the next state value, v_next_vec.\n",
        "    # v_next_vec is a 1D array of shape (batch_size,)\n",
        "    \n",
        "    v_next_vec = None\n",
        "    v_next_vec = np.sum(q_next_mat*probs_mat, 1) * (1 - terminals)\n",
        "    \n",
        "    # Computes Expected Sarsa target\n",
        "    # target_vec is a 1D array of shape (batch_size,)\n",
        "    \n",
        "    target_vec = None\n",
        "    target_vec = rewards + discount * v_next_vec\n",
        "    \n",
        "    # Computes action values at the current states for all actions using network\n",
        "    # q_mat is a 2D array of shape (batch_size, num_actions)\n",
        "    \n",
        "    q_mat = None\n",
        "    q_mat = network.get_action_values(states)\n",
        "    \n",
        "    # Batch Indices is an array from 0 to the batch size - 1. \n",
        "    batch_indices = np.arange(q_mat.shape[0])\n",
        "\n",
        "    # Computes q_vec by selecting q(s, a) from q_mat for taken actions\n",
        "    # q_vec is a 1D array of shape (batch_size)\n",
        "    \n",
        "    q_vec = None\n",
        "    q_vec = q_mat[batch_indices, actions]\n",
        "    \n",
        "    # Computes TD errors for actions taken\n",
        "    # delta_vec is a 1D array of shape (batch_size)\n",
        "    \n",
        "    delta_vec = None\n",
        "    delta_vec = target_vec - q_vec\n",
        "    \n",
        "    return delta_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD13dmJas37e"
      },
      "source": [
        "Testing the implementation of the `get_td_error()` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIBXDBxbs37f",
        "outputId": "a1f8eb9b-3010-4e2a-9728-ee0e4718faad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)\n"
          ]
        }
      ],
      "source": [
        "data = np.load(\"asserts/get_td_error_1.npz\", allow_pickle=True)\n",
        "\n",
        "states = data[\"states\"]\n",
        "next_states = data[\"next_states\"]\n",
        "actions = data[\"actions\"]\n",
        "rewards = data[\"rewards\"]\n",
        "discount = data[\"discount\"]\n",
        "terminals = data[\"terminals\"]\n",
        "tau = 0.001\n",
        "\n",
        "network_config = {\"state_dim\": 8,\n",
        "                  \"num_hidden_units\": 512,\n",
        "                  \"num_actions\": 4\n",
        "                  }\n",
        "\n",
        "network = ActionValueNetwork(network_config)\n",
        "network.set_weights(data[\"network_weights\"])\n",
        "\n",
        "current_q = ActionValueNetwork(network_config)\n",
        "current_q.set_weights(data[\"current_q_weights\"])\n",
        "\n",
        "delta_vec = get_td_error(states, next_states, actions, rewards, discount, terminals, network, current_q, tau)\n",
        "answer_delta_vec = data[\"delta_vec\"]\n",
        "\n",
        "assert(np.allclose(delta_vec, answer_delta_vec))\n",
        "print(\"Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOLaLS90s37f"
      },
      "source": [
        "Now that we implemented the `get_td_error()` function, we used it to implement the `optimize_network()` function. In this function, we:\n",
        "- get the TD-errors vector from `get_td_error()`,\n",
        "- make the TD-errors into a matrix using zeroes for actions not taken in the transitions,\n",
        "- pass the TD-errors matrix to the `get_TD_update()` function of network to calculate the gradients times TD errors, and,\n",
        "- perform an ADAM optimizer step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SQMr6kNks37f"
      },
      "outputs": [],
      "source": [
        "def optimize_network(experiences, discount, optimizer, network, current_q, tau):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        experiences (Numpy array): The batch of experiences including the states, actions, \n",
        "                                   rewards, terminals, and next_states.\n",
        "        discount (float): The discount factor.\n",
        "        network (ActionValueNetwork): The latest state of the network that is getting replay updates.\n",
        "        current_q (ActionValueNetwork): The fixed network used for computing the targets, \n",
        "                                        and particularly, the action-values at the next-states.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Getting states, action, rewards, terminals, and next_states from experiences\n",
        "    states, actions, rewards, terminals, next_states = map(list, zip(*experiences))\n",
        "    states = np.concatenate(states)\n",
        "    next_states = np.concatenate(next_states)\n",
        "    rewards = np.array(rewards)\n",
        "    terminals = np.array(terminals)\n",
        "    batch_size = states.shape[0]\n",
        "\n",
        "    # Computes TD error using the get_td_error function\n",
        "    # q_vec is a 1D array of shape (batch_size)\n",
        "    delta_vec = get_td_error(states, next_states, actions, rewards, discount, terminals, network, current_q, tau)\n",
        "\n",
        "    # Batch Indices is an array from 0 to the batch_size - 1. \n",
        "    batch_indices = np.arange(batch_size)\n",
        "\n",
        "    # Makes a td error matrix of shape (batch_size, num_actions)\n",
        "    # delta_mat has non-zero value only for actions taken\n",
        "    delta_mat = np.zeros((batch_size, network.num_actions))\n",
        "    delta_mat[batch_indices, actions] = delta_vec\n",
        "\n",
        "    # Passing delta_mat to compute the TD errors times the gradients of the network's weights from back-propagation\n",
        "    td_update = None\n",
        "    td_update = network.get_TD_update(states, delta_mat)\n",
        "    \n",
        "    # Passing network.get_weights and the td_update to the optimizer to get updated weights\n",
        "    weights = None\n",
        "    weights = optimizer.update_weights(network.get_weights(), td_update)\n",
        "    \n",
        "    network.set_weights(weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5hJsSrls37f"
      },
      "source": [
        "Testing the implementation of the `optimize_network()` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "e1uTHuLds37g"
      },
      "outputs": [],
      "source": [
        "input_data = np.load(\"asserts/optimize_network_input_1.npz\", allow_pickle=True)\n",
        "\n",
        "experiences = list(input_data[\"experiences\"])\n",
        "discount = input_data[\"discount\"]\n",
        "tau = 0.001\n",
        "\n",
        "network_config = {\"state_dim\": 8,\n",
        "                  \"num_hidden_units\": 512,\n",
        "                  \"num_actions\": 4\n",
        "                  }\n",
        "\n",
        "network = ActionValueNetwork(network_config)\n",
        "network.set_weights(input_data[\"network_weights\"])\n",
        "\n",
        "current_q = ActionValueNetwork(network_config)\n",
        "current_q.set_weights(input_data[\"current_q_weights\"])\n",
        "\n",
        "optimizer_config = {'step_size': 3e-5, \n",
        "                    'beta_m': 0.9, \n",
        "                    'beta_v': 0.999,\n",
        "                    'epsilon': 1e-8\n",
        "                   }\n",
        "optimizer = Adam(network.layer_sizes, optimizer_config)\n",
        "optimizer.m = input_data[\"optimizer_m\"]\n",
        "optimizer.v = input_data[\"optimizer_v\"]\n",
        "optimizer.beta_m_product = input_data[\"optimizer_beta_m_product\"]\n",
        "optimizer.beta_v_product = input_data[\"optimizer_beta_v_product\"]\n",
        "\n",
        "optimize_network(experiences, discount, optimizer, network, current_q, tau)\n",
        "updated_weights = network.get_weights()\n",
        "\n",
        "output_data = np.load(\"asserts/optimize_network_output_1.npz\", allow_pickle=True)\n",
        "answer_updated_weights = output_data[\"updated_weights\"]\n",
        "\n",
        "assert(np.allclose(updated_weights[0][\"W\"], answer_updated_weights[0][\"W\"]))\n",
        "assert(np.allclose(updated_weights[0][\"b\"], answer_updated_weights[0][\"b\"]))\n",
        "assert(np.allclose(updated_weights[1][\"W\"], answer_updated_weights[1][\"W\"]))\n",
        "assert(np.allclose(updated_weights[1][\"b\"], answer_updated_weights[1][\"b\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bU4LDPzHs37g"
      },
      "source": [
        "Now that we implemented the `optimize_network()` function, we implemented the agent. In the cell below, we filled the `agent_step()` and `agent_end()` functions. We:\n",
        "- selected an action (only in `agent_step()`),\n",
        "- add transitions (consisting of the state, action, reward, terminal, and next state) to the replay buffer, and,\n",
        "- updated the weights of the neural network by doing multiple replay steps and calling the `optimize_network()` function that you implemented above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0A825G-As37g"
      },
      "outputs": [],
      "source": [
        "class Agent(BaseAgent):\n",
        "    def __init__(self):\n",
        "        self.name = \"expected_sarsa_agent\"\n",
        "        \n",
        "    def agent_init(self, agent_config):\n",
        "        \"\"\"Setup for the agent called when the experiment first starts.\n",
        "\n",
        "        Set parameters needed to setup the agent.\n",
        "\n",
        "        Assume agent_config dict contains:\n",
        "        {\n",
        "            network_config: dictionary,\n",
        "            optimizer_config: dictionary,\n",
        "            replay_buffer_size: integer,\n",
        "            minibatch_sz: integer, \n",
        "            num_replay_updates_per_step: float\n",
        "            discount_factor: float,\n",
        "        }\n",
        "        \"\"\"\n",
        "        self.replay_buffer = ReplayBuffer(agent_config['replay_buffer_size'], \n",
        "                                          agent_config['minibatch_sz'], agent_config.get(\"seed\"))\n",
        "        self.network = ActionValueNetwork(agent_config['network_config'])\n",
        "        self.optimizer = Adam(self.network.layer_sizes, agent_config[\"optimizer_config\"])\n",
        "        self.num_actions = agent_config['network_config']['num_actions']\n",
        "        self.num_replay = agent_config['num_replay_updates_per_step']\n",
        "        self.discount = agent_config['gamma']\n",
        "        self.tau = agent_config['tau']\n",
        "        \n",
        "        self.rand_generator = np.random.RandomState(agent_config.get(\"seed\"))\n",
        "        \n",
        "        self.last_state = None\n",
        "        self.last_action = None\n",
        "        \n",
        "        self.sum_rewards = 0\n",
        "        self.episode_steps = 0\n",
        "\n",
        "    def policy(self, state):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            state (Numpy array): the state.\n",
        "        Returns:\n",
        "            the action. \n",
        "        \"\"\"\n",
        "        action_values = self.network.get_action_values(state)\n",
        "        probs_batch = softmax(action_values, self.tau)\n",
        "        action = self.rand_generator.choice(self.num_actions, p=probs_batch.squeeze())\n",
        "        return action\n",
        "\n",
        "    def agent_start(self, state):\n",
        "        \"\"\"The first method called when the experiment starts, called after\n",
        "        the environment starts.\n",
        "        Args:\n",
        "            state (Numpy array): the state from the\n",
        "                environment's evn_start function.\n",
        "        Returns:\n",
        "            The first action the agent takes.\n",
        "        \"\"\"\n",
        "        self.sum_rewards = 0\n",
        "        self.episode_steps = 0\n",
        "        self.last_state = np.array([state])\n",
        "        self.last_action = self.policy(self.last_state)\n",
        "        return self.last_action\n",
        "\n",
        "    # weights update using optimize_network, and updating last_state and last_action (~5 lines).\n",
        "    def agent_step(self, reward, state):\n",
        "        \"\"\"A step taken by the agent.\n",
        "        Args:\n",
        "            reward (float): the reward received for taking the last action taken\n",
        "            state (Numpy array): the state from the\n",
        "                environment's step based, where the agent ended up after the\n",
        "                last step\n",
        "        Returns:\n",
        "            The action the agent is taking.\n",
        "        \"\"\"\n",
        "        \n",
        "        self.sum_rewards += reward\n",
        "        self.episode_steps += 1\n",
        "\n",
        "        # Makes state an array of shape (1, state_dim) to add a batch dimension and\n",
        "        # to later match the get_action_values() and get_TD_update() functions\n",
        "        state = np.array([state])\n",
        "\n",
        "        action = self.policy(state)\n",
        "        \n",
        "        # Appended new experience to replay buffer\n",
        "\n",
        "        self.replay_buffer.append(self.last_state, self.last_action, reward, 0, state)\n",
        "        \n",
        "        # Performing replay steps:\n",
        "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
        "            current_q = deepcopy(self.network)\n",
        "            for _ in range(self.num_replay):\n",
        "                \n",
        "                # Getting sample experiences from the replay buffer\n",
        "                experiences = self.replay_buffer.sample()\n",
        "                \n",
        "                # Calling optimize_network to update the weights of the network (~1 Line)\n",
        "                optimize_network(experiences, self.discount, self.optimizer, self.network, current_q, self.tau) \n",
        "                \n",
        "\n",
        "        self.last_state = None\n",
        "        self.last_action = None\n",
        "\n",
        "        self.last_state = state\n",
        "        self.last_action = action\n",
        "        \n",
        "        return action\n",
        "\n",
        "    def agent_end(self, reward):\n",
        "        \"\"\"Run when the agent terminates.\n",
        "        Args:\n",
        "            reward (float): the reward the agent received for entering the\n",
        "                terminal state.\n",
        "        \"\"\"\n",
        "        self.sum_rewards += reward\n",
        "        self.episode_steps += 1\n",
        "        \n",
        "        # Setting terminal state to an array of zeros\n",
        "        state = np.zeros_like(self.last_state)\n",
        "\n",
        "        # Appending new experience to replay buffer\n",
        "        self.replay_buffer.append(self.last_state, self.last_action, reward, 1, state)\n",
        "        \n",
        "        # Performing replay steps:\n",
        "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
        "            current_q = deepcopy(self.network)\n",
        "            for _ in range(self.num_replay):\n",
        "                \n",
        "                # Getting sample experiences from the replay buffer\n",
        "                experiences = self.replay_buffer.sample()\n",
        "                \n",
        "                # Calling optimize_network to update the weights of the network\n",
        "                optimize_network(experiences, self.discount, self.optimizer, self.network, current_q, self.tau) \n",
        "                \n",
        "        \n",
        "    def agent_message(self, message):\n",
        "        if message == \"get_sum_reward\":\n",
        "            return self.sum_rewards\n",
        "        else:\n",
        "            raise Exception(\"Unrecognized Message!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJHQsnQ1s37h"
      },
      "source": [
        "Testing the implementation of the `agent_step()` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raEm1luZs37h",
        "outputId": "44819cc5-718e-4774-bf02-c851ecba6efe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:64: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        }
      ],
      "source": [
        "agent_info = {\n",
        "             'network_config': {\n",
        "                 'state_dim': 8,\n",
        "                 'num_hidden_units': 256,\n",
        "                 'num_hidden_layers': 1,\n",
        "                 'num_actions': 4\n",
        "             },\n",
        "             'optimizer_config': {\n",
        "                 'step_size': 3e-5, \n",
        "                 'beta_m': 0.9, \n",
        "                 'beta_v': 0.999,\n",
        "                 'epsilon': 1e-8\n",
        "             },\n",
        "             'replay_buffer_size': 32,\n",
        "             'minibatch_sz': 32,\n",
        "             'num_replay_updates_per_step': 4,\n",
        "             'gamma': 0.99,\n",
        "             'tau': 1000.0,\n",
        "             'seed': 0}\n",
        "\n",
        "# Initialize agent\n",
        "agent = Agent()\n",
        "agent.agent_init(agent_info)\n",
        "\n",
        "# load agent network, optimizer, replay_buffer from the agent_input_1.npz file\n",
        "input_data = np.load(\"asserts/agent_input_1.npz\", allow_pickle=True)\n",
        "agent.network.set_weights(input_data[\"network_weights\"])\n",
        "agent.optimizer.m = input_data[\"optimizer_m\"]\n",
        "agent.optimizer.v = input_data[\"optimizer_v\"]\n",
        "agent.optimizer.beta_m_product = input_data[\"optimizer_beta_m_product\"]\n",
        "agent.optimizer.beta_v_product = input_data[\"optimizer_beta_v_product\"]\n",
        "agent.replay_buffer.rand_generator.seed(int(input_data[\"replay_buffer_seed\"]))\n",
        "for experience in input_data[\"replay_buffer\"]:\n",
        "    agent.replay_buffer.buffer.append(experience)\n",
        "\n",
        "# Perform agent_step multiple times\n",
        "last_state_array = input_data[\"last_state_array\"]\n",
        "last_action_array = input_data[\"last_action_array\"]\n",
        "state_array = input_data[\"state_array\"]\n",
        "reward_array = input_data[\"reward_array\"]\n",
        "\n",
        "for i in range(5):\n",
        "    agent.last_state = last_state_array[i]\n",
        "    agent.last_action = last_action_array[i]\n",
        "    state = state_array[i]\n",
        "    reward = reward_array[i]\n",
        "    \n",
        "    agent.agent_step(reward, state)\n",
        "    \n",
        "    # Load expected values for last_state, last_action, weights, and replay_buffer \n",
        "    output_data = np.load(\"asserts/agent_step_output_{}.npz\".format(i), allow_pickle=True)\n",
        "    answer_last_state = output_data[\"last_state\"]\n",
        "    answer_last_action = output_data[\"last_action\"]\n",
        "    answer_updated_weights = output_data[\"updated_weights\"]\n",
        "    answer_replay_buffer = output_data[\"replay_buffer\"]\n",
        "\n",
        "    # Asserts for last_state and last_action\n",
        "    assert(np.allclose(answer_last_state, agent.last_state))\n",
        "    assert(np.allclose(answer_last_action, agent.last_action))\n",
        "\n",
        "    # Asserts for replay_buffer \n",
        "    for i in range(answer_replay_buffer.shape[0]):\n",
        "        for j in range(answer_replay_buffer.shape[1]):\n",
        "            assert(np.allclose(np.asarray(agent.replay_buffer.buffer)[i, j], answer_replay_buffer[i, j]))\n",
        "\n",
        "    # Asserts for network.weights\n",
        "    assert(np.allclose(agent.network.weights[0][\"W\"], answer_updated_weights[0][\"W\"]))\n",
        "    assert(np.allclose(agent.network.weights[0][\"b\"], answer_updated_weights[0][\"b\"]))\n",
        "    assert(np.allclose(agent.network.weights[1][\"W\"], answer_updated_weights[1][\"W\"]))\n",
        "    assert(np.allclose(agent.network.weights[1][\"b\"], answer_updated_weights[1][\"b\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeTGNpRWs37h"
      },
      "source": [
        "Testing the implementation of the `agent_end()` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8878Orxms37h",
        "outputId": "8e7d3094-8cb2-4fd8-e338-929ba0ed9e29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:58: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        }
      ],
      "source": [
        "agent_info = {\n",
        "             'network_config': {\n",
        "                 'state_dim': 8,\n",
        "                 'num_hidden_units': 256,\n",
        "                 'num_hidden_layers': 1,\n",
        "                 'num_actions': 4\n",
        "             },\n",
        "             'optimizer_config': {\n",
        "                 'step_size': 3e-5, \n",
        "                 'beta_m': 0.9, \n",
        "                 'beta_v': 0.999,\n",
        "                 'epsilon': 1e-8\n",
        "             },\n",
        "             'replay_buffer_size': 32,\n",
        "             'minibatch_sz': 32,\n",
        "             'num_replay_updates_per_step': 4,\n",
        "             'gamma': 0.99,\n",
        "             'tau': 1000,\n",
        "             'seed': 0\n",
        "             }\n",
        "\n",
        "# Initialize agent\n",
        "agent = Agent()\n",
        "agent.agent_init(agent_info)\n",
        "\n",
        "# load agent network, optimizer, replay_buffer from the agent_input_1.npz file\n",
        "input_data = np.load(\"asserts/agent_input_1.npz\", allow_pickle=True)\n",
        "agent.network.set_weights(input_data[\"network_weights\"])\n",
        "agent.optimizer.m = input_data[\"optimizer_m\"]\n",
        "agent.optimizer.v = input_data[\"optimizer_v\"]\n",
        "agent.optimizer.beta_m_product = input_data[\"optimizer_beta_m_product\"]\n",
        "agent.optimizer.beta_v_product = input_data[\"optimizer_beta_v_product\"]\n",
        "agent.replay_buffer.rand_generator.seed(int(input_data[\"replay_buffer_seed\"]))\n",
        "for experience in input_data[\"replay_buffer\"]:\n",
        "    agent.replay_buffer.buffer.append(experience)\n",
        "\n",
        "# Perform agent_step multiple times\n",
        "last_state_array = input_data[\"last_state_array\"]\n",
        "last_action_array = input_data[\"last_action_array\"]\n",
        "state_array = input_data[\"state_array\"]\n",
        "reward_array = input_data[\"reward_array\"]\n",
        "\n",
        "for i in range(5):\n",
        "    agent.last_state = last_state_array[i]\n",
        "    agent.last_action = last_action_array[i]\n",
        "    reward = reward_array[i]\n",
        "    \n",
        "    agent.agent_end(reward)\n",
        "\n",
        "    # Load expected values for last_state, last_action, weights, and replay_buffer \n",
        "    output_data = np.load(\"asserts/agent_end_output_{}.npz\".format(i), allow_pickle=True)\n",
        "    answer_updated_weights = output_data[\"updated_weights\"]\n",
        "    answer_replay_buffer = output_data[\"replay_buffer\"]\n",
        "\n",
        "    # Asserts for replay_buffer \n",
        "    for i in range(answer_replay_buffer.shape[0]):\n",
        "        for j in range(answer_replay_buffer.shape[1]):\n",
        "            assert(np.allclose(np.asarray(agent.replay_buffer.buffer)[i, j], answer_replay_buffer[i, j]))\n",
        "\n",
        "    # Asserts for network.weights\n",
        "    assert(np.allclose(agent.network.weights[0][\"W\"], answer_updated_weights[0][\"W\"]))\n",
        "    assert(np.allclose(agent.network.weights[0][\"b\"], answer_updated_weights[0][\"b\"]))\n",
        "    assert(np.allclose(agent.network.weights[1][\"W\"], answer_updated_weights[1][\"W\"]))\n",
        "    assert(np.allclose(agent.network.weights[1][\"b\"], answer_updated_weights[1][\"b\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu-QjEv7s37i"
      },
      "source": [
        "## Run Experiment\n",
        "\n",
        "We plotted the learning curve of the agent to visualize learning progress. We used the sum of rewards in an episode as the performance measure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ag8ULwcBs37i",
        "outputId": "dcc28eda-69b1-46e0-d51d-b7c04f49f57b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:269: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  \"Function `env.seed(seed)` is marked as deprecated and will be removed in the future. \"\n",
            "100%|| 300/300 [08:19<00:00,  1.66s/it]\n"
          ]
        }
      ],
      "source": [
        "def run_experiment(environment, agent, environment_parameters, agent_parameters, experiment_parameters):\n",
        "    \n",
        "    rl_glue = RLGlue(environment, agent)\n",
        "        \n",
        "    # save sum of reward at the end of each episode\n",
        "    agent_sum_reward = np.zeros((experiment_parameters[\"num_runs\"], \n",
        "                                 experiment_parameters[\"num_episodes\"]))\n",
        "\n",
        "    env_info = {}\n",
        "\n",
        "    agent_info = agent_parameters\n",
        "\n",
        "    # one agent setting\n",
        "    for run in range(1, experiment_parameters[\"num_runs\"]+1):\n",
        "        agent_info[\"seed\"] = run\n",
        "        agent_info[\"network_config\"][\"seed\"] = run\n",
        "        env_info[\"seed\"] = run\n",
        "\n",
        "        rl_glue.rl_init(agent_info, env_info)\n",
        "        \n",
        "        for episode in tqdm(range(1, experiment_parameters[\"num_episodes\"]+1)):\n",
        "            # run episode\n",
        "            rl_glue.rl_episode(experiment_parameters[\"timeout\"])\n",
        "            \n",
        "            episode_reward = rl_glue.rl_agent_message(\"get_sum_reward\")\n",
        "            agent_sum_reward[run - 1, episode - 1] = episode_reward\n",
        "    save_name = \"{}\".format(rl_glue.agent.name)\n",
        "    if not os.path.exists('results'):\n",
        "        os.makedirs('results')\n",
        "    np.save(\"results/sum_reward_{}\".format(save_name), agent_sum_reward)\n",
        "    shutil.make_archive('results', 'zip', 'results')\n",
        "\n",
        "# Run Experiment\n",
        "\n",
        "# Experiment parameters\n",
        "experiment_parameters = {\n",
        "    \"num_runs\" : 1,\n",
        "    \"num_episodes\" : 300,\n",
        "    # OpenAI Gym environments allow for a timestep limit timeout, causing episodes to end after \n",
        "    # some number of timesteps. Here we use the default of 500.\n",
        "    \"timeout\" : 500\n",
        "}\n",
        "\n",
        "# Environment parameters\n",
        "environment_parameters = {}\n",
        "\n",
        "current_env = MoonLanderEnvironment\n",
        "\n",
        "# Agent parameters\n",
        "agent_parameters = {\n",
        "    'network_config': {\n",
        "        'state_dim': 8,\n",
        "        'num_hidden_units': 256,\n",
        "        'num_actions': 4\n",
        "    },\n",
        "    'optimizer_config': {\n",
        "        'step_size': 1e-3,\n",
        "        'beta_m': 0.9, \n",
        "        'beta_v': 0.999,\n",
        "        'epsilon': 1e-8\n",
        "    },\n",
        "    'replay_buffer_size': 50000,\n",
        "    'minibatch_sz': 8,\n",
        "    'num_replay_updates_per_step': 4,\n",
        "    'gamma': 0.99,\n",
        "    'tau': 0.001\n",
        "}\n",
        "current_agent = Agent\n",
        "\n",
        "# run experiment\n",
        "run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison between our implemented agent and a random agent for one run and 300 episodes"
      ],
      "metadata": {
        "id": "wD91SaYkvG2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_result([\"expected_sarsa_agent\", \"random_agent\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "G_DAGc4muNDy",
        "outputId": "893b2d9e-bc05-473e-b9ee-52d80acdb71e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameter Study"
      ],
      "metadata": {
        "id": "NcUQQxRIwufx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(environment, agent, environment_parameters, agent_parameters, experiment_parameters):\n",
        "    \n",
        "    \"\"\"\n",
        "    Assume environment_parameters dict contains:\n",
        "    {\n",
        "        input_dim: integer,\n",
        "        num_actions: integer,\n",
        "        discount_factor: float\n",
        "    }\n",
        "    \n",
        "    Assume agent_parameters dict contains:\n",
        "    {\n",
        "        step_size: 1D numpy array of floats,\n",
        "        tau: 1D numpy array of floats\n",
        "    }\n",
        "    \n",
        "    Assume experiment_parameters dict contains:\n",
        "    {\n",
        "        num_runs: integer,\n",
        "        num_episodes: integer\n",
        "    }    \n",
        "    \"\"\"\n",
        "    \n",
        "    ### Instantiate rl_glue from RLGlue    \n",
        "    rl_glue = RLGlue(environment, agent)\n",
        "\n",
        "    os.system('sleep 1') # to prevent tqdm printing out-of-order\n",
        "    \n",
        "    \n",
        "    ### Initialize agent_sum_reward to zero in the form of a numpy array \n",
        "    # with shape (number of values for tau, number of step-sizes, number of runs, number of episodes)\n",
        "    agent_sum_reward = None\n",
        "\n",
        "    agent_sum_reward = np.zeros((len(agent_parameters['tau']), \n",
        "                                 len(agent_parameters['step_size']), \n",
        "                                 experiment_parameters['num_runs'], \n",
        "                                 experiment_parameters['num_episodes']))\n",
        "    \n",
        "    \n",
        "    \n",
        "    # for loop over different values of tau\n",
        "    # tqdm is used to show a progress bar for completing the parameter study\n",
        "    for i in tqdm(range(len(agent_parameters['tau']))):\n",
        "        \n",
        "        # for loop over different values of the step-size\n",
        "        for j in range(len(agent_parameters['step_size'])):\n",
        "\n",
        "            ### Specify env_info \n",
        "            env_info = environment_parameters\n",
        "\n",
        "            ### Specify agent_info\n",
        "            agent_info = {\"num_actions\": environment_parameters[\"num_actions\"],\n",
        "                          \"input_dim\": environment_parameters[\"input_dim\"],\n",
        "                          \"discount_factor\": environment_parameters[\"discount_factor\"],\n",
        "                          \"tau\": agent_parameters['tau'][i],\n",
        "                          \"step_size\": agent_parameters['step_size'][j],\n",
        "                         }\n",
        "            \n",
        "            for run in range(experiment_parameters['num_runs']):\n",
        "                \n",
        "                # Set the seed\n",
        "                agent_info[\"seed\"] = agent_parameters[\"seed\"] * experiment_parameters[\"num_runs\"] + run\n",
        "                \n",
        "                # Beginning of the run            \n",
        "                rl_glue.rl_init(agent_info, env_info)\n",
        "\n",
        "                for episode in range(experiment_parameters['num_episodes']):\n",
        "                    \n",
        "                    # Run episode\n",
        "                    rl_glue.rl_episode(0) # no step limit\n",
        "\n",
        "                    ### Store sum of reward\n",
        "                    # agent_sum_reward[None, None, None, None] = rl_glue.rl_agent_message(\"get_sum_reward\")\n",
        "                    agent_sum_reward[i, j, run, episode] = rl_glue.rl_agent_message(\"get_sum_reward\")\n",
        "\n",
        "            if not os.path.exists('results'):\n",
        "                    os.makedirs('results')\n",
        "\n",
        "            save_name = \"{}\".format(rl_glue.agent.name).replace('.','')\n",
        "\n",
        "            # save sum reward\n",
        "            np.save(\"results/sum_reward_{}\".format(save_name), agent_sum_reward) "
      ],
      "metadata": {
        "id": "iGnfFcI9wLvv"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the implementation of `run_experiment()` given a dummy agent and a dummy environment for 100 runs, 100 episodes, 12 values of the step-size, and 4 values of $\\tau$:"
      ],
      "metadata": {
        "id": "12t3gkbmxBh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment parameters\n",
        "experiment_parameters = {\n",
        "    \"num_runs\" : 100,\n",
        "    \"num_episodes\" : 100,\n",
        "}\n",
        "\n",
        "# Environment parameters\n",
        "environment_parameters = {\n",
        "    \"input_dim\" : 8,\n",
        "    \"num_actions\": 4, \n",
        "    \"discount_factor\" : 0.99\n",
        "}\n",
        "\n",
        "agent_parameters = {\n",
        "    \"step_size\": 3e-5 * np.power(2.0, np.array([-6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5])),\n",
        "    \"tau\": np.array([0.001, 0.01, 0.1, 1.0]),\n",
        "    \"seed\": 0\n",
        "}\n",
        "\n",
        "test_env = DummyEnvironment\n",
        "test_agent = DummyAgent\n",
        "\n",
        "run_experiment(test_env, \n",
        "               test_agent, \n",
        "               environment_parameters, \n",
        "               agent_parameters, \n",
        "               experiment_parameters)\n",
        "\n",
        "sum_reward_dummy_agent = np.load(\"results/sum_reward_dummy_agent.npy\")\n",
        "sum_reward_dummy_agent_answer = np.load(\"asserts/sum_reward_dummy_agent.npy\")\n",
        "assert(np.allclose(sum_reward_dummy_agent, sum_reward_dummy_agent_answer))\n",
        "\n",
        "print(\"\\nPassed the assert!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RG9i_u9w9mZ",
        "outputId": "641183e2-c3ff-4878-dcdb-b792426d5be1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 4/4 [00:08<00:00,  2.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Passed the assert!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_parameters = {\n",
        "    \"num_runs\" : 100,\n",
        "    \"num_episodes\" : 100,\n",
        "}\n",
        "\n",
        "# Environment parameters\n",
        "environment_parameters = {\n",
        "    \"input_dim\" : 8,\n",
        "    \"num_actions\": 4, \n",
        "    \"discount_factor\" : 0.99\n",
        "}\n",
        "\n",
        "agent_parameters = {\n",
        "    \"step_size\": 3e-5 * np.power(2.0, np.array([-6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5])),\n",
        "    \"tau\": np.array([0.001, 0.01, 0.1, 1.0]),\n",
        "    \"seed\": 0\n",
        "}\n",
        "\n",
        "test_env = DummyEnvironment\n",
        "test_agent = DummyAgent\n",
        "\n",
        "run_experiment(test_env, \n",
        "               test_agent,  \n",
        "               environment_parameters, \n",
        "               agent_parameters, \n",
        "               experiment_parameters)\n",
        "\n",
        "sum_reward_dummy_agent = np.load(\"results/sum_reward_dummy_agent.npy\")\n",
        "sum_reward_dummy_agent_answer = np.load(\"asserts/sum_reward_dummy_agent.npy\")\n",
        "assert(np.allclose(sum_reward_dummy_agent, sum_reward_dummy_agent_answer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UgEWtacycsR",
        "outputId": "0b2dd5ea-1030-4bb9-bf47-6f5b19b370e9"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 4/4 [00:09<00:00,  2.39s/it]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.5 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "bd3f1da8f6b2e069fa79ea943cb28ac35c78b055477c15e9cfb0a47fe444e79b"
      }
    },
    "colab": {
      "name": "MoonLander.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}